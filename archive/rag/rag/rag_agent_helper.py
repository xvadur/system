#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG Agent Helper: Wrapper pre Cursor agenta na automatick√© vyhƒæad√°vanie v RAG indexe.

Tento skript m√¥≈æe by≈• volan√Ω z Cursor agenta cez terminal command.
Vracia JSON v√Ωsledky, ktor√© m√¥≈æe agent pou≈æi≈• v odpovedi.
"""

import json
import sys
import re
from pathlib import Path
from typing import List, Dict, Optional
from collections import Counter

try:
    import faiss
    import numpy as np
    from openai import OpenAI
except ImportError:
    print(json.dumps({"error": "Missing dependencies: faiss-cpu, numpy, openai"}))
    sys.exit(1)

import os

# Konfigur√°cia
INDEX_DIR = Path("data/rag_index")
EMBEDDING_MODEL = "qwen/qwen3-embedding-8b"  # Qwen3 Embedding 8B cez OpenRouter
EMBEDDING_DIM = 4096  # qwen3-embedding-8b m√° 4096 dimenzi√≠ (cez OpenRouter)
# Pozn√°mka: Star√Ω index m√¥≈æe ma≈• 1536 dimenzi√≠ (OpenAI text-embedding-3-small)
# Skript automaticky detekuje dimenzie indexu
USE_OPENROUTER = True  # Pou≈æi≈• OpenRouter API namiesto OpenAI
OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"  # OpenRouter API endpoint

# Naƒç√≠tanie API key z .env s√∫boru alebo environmentu
def load_api_key():
    """Naƒç√≠ta OpenRouter API key z .env s√∫boru alebo environmentu."""
    # Priorita: OpenRouter keys
    api_key = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPEROUTER_API_KEY") or os.getenv("OPEN_ROUTER")
    if api_key:
        return api_key
    
    env_files = [
        Path(".env"),
        Path("mcp/.env")
    ]
    
    for env_file in env_files:
        if env_file.exists():
            try:
                # Najprv preƒç√≠taj cel√Ω s√∫bor a hƒæadaj OpenRouter key
                with open(env_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                # Hƒæadaj len OpenRouter keys
                for line in lines:
                    line = line.strip()
                    if (line.startswith("OPENROUTER_API_KEY=") or 
                        line.startswith("OPEROUTER_API_KEY=") or 
                        line.startswith("OPEN_ROUTER=")):
                        key = line.split("=", 1)[1].strip()
                        key = key.strip('"').strip("'")
                        if key and key != "changeme":
                            return key
            except Exception:
                continue
    
    return None


def load_index(index_dir: Path) -> tuple:
    """Naƒç√≠ta FAISS index a metadata."""
    index_path = index_dir / "faiss.index"
    metadata_path = index_dir / "metadata.json"
    chunks_path = index_dir / "chunks.json"
    
    if not index_path.exists():
        return None, None, None, None
    
    index = faiss.read_index(str(index_path))
    
    # Detekcia dimenzi√≠ indexu (ulo≈æ√≠me do glob√°lnej premennej pre pou≈æitie v search_rag)
    actual_dim = index.d
    global EMBEDDING_DIM
    if actual_dim != EMBEDDING_DIM:
        # Index m√° in√© dimenzie - uprav√≠me glob√°lnu premenn√∫
        EMBEDDING_DIM = actual_dim
    
    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    with open(chunks_path, 'r', encoding='utf-8') as f:
        chunks = json.load(f)
    
    # Vytvorenie TF-IDF indexu pre keyword search
    tfidf_index = build_tfidf_index(chunks)
    
    return index, metadata, chunks, tfidf_index


def tokenize(text: str) -> List[str]:
    """Tokenizuje text na slov√° (slovenƒçina + angliƒçtina)."""
    # Odstr√°nenie diakritiky a lowercase
    text = text.lower()
    # Extrahovanie slov (p√≠smen√°, ƒç√≠sla, podƒçiarkovn√≠ky)
    words = re.findall(r'\b\w+\b', text)
    return words


def build_tfidf_index(chunks: List[str]) -> Dict:
    """
    Vytvor√≠ jednoduch√Ω TF-IDF index pre keyword search.
    
    Returns:
        Dict s term frequencies a document frequencies
    """
    # Poƒçet dokumentov
    num_docs = len(chunks)
    
    # Term frequency pre ka≈æd√Ω dokument
    doc_term_freqs = []
    # Document frequency pre ka≈æd√Ω term
    doc_freq = Counter()
    
    for chunk in chunks:
        tokens = tokenize(chunk)
        term_freq = Counter(tokens)
        doc_term_freqs.append(term_freq)
        
        # Poƒç√≠tanie document frequency (v koƒæk√Ωch dokumentoch sa term vyskytuje)
        unique_terms = set(tokens)
        for term in unique_terms:
            doc_freq[term] += 1
    
    return {
        "doc_term_freqs": doc_term_freqs,
        "doc_freq": doc_freq,
        "num_docs": num_docs
    }


def keyword_search(
    query: str,
    chunks: List[str],
    tfidf_index: Dict,
    top_k: int = 10
) -> List[Dict]:
    """
    Keyword search pomocou TF-IDF.
    
    Args:
        query: Vyhƒæad√°vac√≠ dotaz
        chunks: Text chunkov
        tfidf_index: TF-IDF index
        top_k: Poƒçet v√Ωsledkov
    
    Returns:
        List of search results with TF-IDF scores
    """
    query_tokens = tokenize(query)
    if not query_tokens:
        return []
    
    doc_term_freqs = tfidf_index["doc_term_freqs"]
    doc_freq = tfidf_index["doc_freq"]
    num_docs = tfidf_index["num_docs"]
    
    scores = []
    
    for idx, chunk in enumerate(chunks):
        if idx >= len(doc_term_freqs):
            continue
        
        term_freq = doc_term_freqs[idx]
        score = 0.0
        
        # TF-IDF v√Ωpoƒçet pre ka≈æd√Ω query token
        for token in query_tokens:
            # Term Frequency (TF)
            tf = term_freq.get(token, 0)
            if tf == 0:
                continue
            
            # Inverse Document Frequency (IDF)
            df = doc_freq.get(token, 0)
            if df == 0:
                continue
            
            idf = np.log(num_docs / (1 + df))
            
            # TF-IDF score
            score += tf * idf
        
        if score > 0:
            scores.append({
                "idx": idx,
                "score": score,
                "text": chunk
            })
    
    # Zoradenie podƒæa score
    scores.sort(key=lambda x: x["score"], reverse=True)
    
    # Vr√°tenie top-k v√Ωsledkov
    return scores[:top_k]


def search_rag(
    query: str,
    top_k: int = 5,
    min_score: float = 0.4,
    use_hybrid: bool = True,
    semantic_weight: float = 0.7,
    keyword_weight: float = 0.3,
    content_type_filter: Optional[str] = None,
    index_dir: Optional[Path] = None
) -> List[Dict]:
    """
    Hybrid search v RAG indexe - kombinuje semantic (embeddings) a keyword (TF-IDF) search.
    
    Args:
        query: Vyhƒæad√°vac√≠ dotaz
        top_k: Poƒçet v√Ωsledkov
        min_score: Minim√°lne similarity score (0-1)
        use_hybrid: Pou≈æi≈• hybrid search (True) alebo len semantic (False)
        semantic_weight: V√°ha semantic search (0-1)
        keyword_weight: V√°ha keyword search (0-1)
        content_type_filter: Filter podƒæa content_type ("prompt", "response", "pair", alebo None pre v≈°etko)
    
    Returns:
        List of search results with metadata
    """
    # Naƒç√≠tanie API key (OpenRouter)
    api_key = load_api_key()
    if not api_key:
        return [{"error": "OPENROUTER_API_KEY nie je nastaven√Ω. Nastav OPENROUTER_API_KEY v .env alebo environmente."}]
    
    # Naƒç√≠tanie indexu (pou≈æi poskytnut√Ω index_dir alebo default)
    index_path = index_dir if index_dir else INDEX_DIR
    index, metadata, chunks, tfidf_index = load_index(index_path)
    if index is None:
        return [{"error": f"RAG index neexistuje v {index_path}. Spusti najprv build_rag_index.py"}]
    
    actual_dim = index.d
    
    # V≈ædy pou≈æ√≠vame OpenRouter s qwen modelom
    embedding_model = EMBEDDING_MODEL
    use_openrouter_for_emb = USE_OPENROUTER
    
    # Ak index m√° in√© dimenzie, treba ho rebuildn√∫≈•
    # Ak index m√° in√© dimenzie, treba ho rebuildn√∫≈•
    if actual_dim != EMBEDDING_DIM:
        return [{"error": f"Nezhoda dimenzi√≠: index m√° {actual_dim}, qwen/qwen3-embedding-8b m√° {EMBEDDING_DIM}. Treba rebuildn√∫≈• index:\n  python3 archive/rag/rag/build_rag_index.py"}]
    
    # SEMANTIC SEARCH (embeddings) - V≈ΩDY pou≈æ√≠vame OpenRouter
    # OpenAI SDK je kompatibiln√© s OpenRouter API
    client = OpenAI(
        api_key=api_key, 
        base_url=OPENROUTER_BASE_URL,
        default_headers={
            "HTTP-Referer": "https://github.com/xvadur/system",
            "X-Title": "RAG Agent Helper"
        }
    )
    try:
        response = client.embeddings.create(
            model=embedding_model,
            input=[query]
        )
        query_embedding = np.array([response.data[0].embedding], dtype=np.float32)
        
        # Kontrola dimenzi√≠
        if query_embedding.shape[1] != actual_dim:
            return [{"error": f"Nezhoda dimenzi√≠: embedding m√° {query_embedding.shape[1]}, index m√° {actual_dim}"}]
    except Exception as e:
        return [{"error": f"Chyba pri generovan√≠ embeddingu: {str(e)}"}]
    
    # Semantic search - vezmeme viac v√Ωsledkov pre hybrid kombin√°ciu
    search_k = top_k * 3 if use_hybrid else top_k
    distances, semantic_indices = index.search(query_embedding, search_k)
    
    semantic_results = {}
    for distance, idx in zip(distances[0], semantic_indices[0]):
        if idx < len(chunks) and idx < len(metadata):
            score = float(1 / (1 + distance))
            semantic_results[idx] = {
                "score": score,
                "distance": float(distance)
            }
    
    # KEYWORD SEARCH (TF-IDF)
    keyword_results = {}
    if use_hybrid and tfidf_index:
        keyword_matches = keyword_search(query, chunks, tfidf_index, top_k=search_k)
        
        # Normaliz√°cia TF-IDF sk√≥re (0-1)
        if keyword_matches:
            max_tfidf = max(m["score"] for m in keyword_matches)
            if max_tfidf > 0:
                for match in keyword_matches:
                    idx = match["idx"]
                    normalized_score = match["score"] / max_tfidf
                    keyword_results[idx] = {
                        "score": normalized_score,
                        "tfidf": match["score"]
                    }
    
    # HYBRID KOMBIN√ÅCIA
    if use_hybrid and keyword_results:
        # Kombin√°cia semantic + keyword v√Ωsledkov
        all_indices = set(semantic_results.keys()) | set(keyword_results.keys())
        hybrid_scores = {}
        
        for idx in all_indices:
            semantic_score = semantic_results.get(idx, {}).get("score", 0.0)
            keyword_score = keyword_results.get(idx, {}).get("score", 0.0)
            
            # V√°≈æen√Ω priemer
            hybrid_score = (semantic_score * semantic_weight) + (keyword_score * keyword_weight)
            hybrid_scores[idx] = {
                "hybrid_score": hybrid_score,
                "semantic_score": semantic_score,
                "keyword_score": keyword_score
            }
        
        # Zoradenie podƒæa hybrid score
        sorted_indices = sorted(
            hybrid_scores.items(),
            key=lambda x: x[1]["hybrid_score"],
            reverse=True
        )
    else:
        # Len semantic search
        sorted_indices = sorted(
            semantic_results.items(),
            key=lambda x: x[1]["score"],
            reverse=True
        )
        # Prekonvertovanie na rovnak√Ω form√°t
        sorted_indices = [
            (idx, {
                "hybrid_score": data["score"],
                "semantic_score": data["score"],
                "keyword_score": 0.0
            })
            for idx, data in sorted_indices
        ]
    
    # Zostavenie v√Ωsledkov
    results = []
    for rank, (idx, scores) in enumerate(sorted_indices[:top_k], 1):
        if idx >= len(chunks) or idx >= len(metadata):
            continue
        
        hybrid_score = scores["hybrid_score"]
        
        # Filtrovanie podƒæa minim√°lneho score
        if hybrid_score < min_score:
            continue
        
        meta = metadata[idx]
        
        # Filtrovanie podƒæa content_type
        content_type = meta.get("content_type", "prompt")
        if content_type_filter and content_type != content_type_filter:
            continue
        
        results.append({
            "rank": rank,
            "score": hybrid_score,
            "semantic_score": scores["semantic_score"],
            "keyword_score": scores["keyword_score"],
            "text": chunks[idx],
            "date": meta.get("date", "N/A"),
            "timestamp": meta.get("timestamp", "N/A"),
            "source_path": meta.get("source_path", "N/A"),
            "chunk_index": meta.get("chunk_index", 0),
            "total_chunks": meta.get("total_chunks", 1),
            "content_type": content_type,
            "search_type": "hybrid" if use_hybrid and keyword_results else "semantic"
        })
    
    return results


def query_rag_with_synthesis(
    query: str,
    top_k: int = 10,
    min_score: float = 0.4,
    use_hybrid: bool = True,
    model: str = "deepseek/deepseek-v3.2",
    temperature: float = 0.3
) -> Dict:
    """
    RAG Query s automatickou synt√©zou odpovede.
    
    Namiesto surov√Ωch promptov vr√°ti syntetizovan√∫ odpoveƒè, ktor√° obsahuje
    hlavn√© inform√°cie z relevantn√Ωch promptov.
    
    Args:
        query: Vyhƒæad√°vac√≠ dotaz
        top_k: Poƒçet v√Ωsledkov pre synt√©zu
        min_score: Minim√°lne similarity score
        use_hybrid: Pou≈æi≈• hybrid search
        model: LLM model pre synt√©zu cez OpenRouter (napr. deepseek/deepseek-v3.2)
        temperature: Temperature pre LLM
    
    Returns:
        Dict s syntetizovanou odpoveƒèou a metad√°tami
    """
    # 1. RAG Search - n√°jde relevantn√© prompty
    rag_results = search_rag(query, top_k=top_k, min_score=min_score, use_hybrid=use_hybrid)
    
    if not rag_results or "error" in rag_results[0]:
        return {
            "query": query,
            "synthesized_answer": "Nena≈°li sa relevantn√© v√Ωsledky v RAG indexe.",
            "sources_count": 0,
            "sources": []
        }
    
    # 2. Zostavenie kontextu z v√Ωsledkov
    context_parts = []
    sources = []
    
    for result in rag_results[:top_k]:
        date = result.get("date", "N/A")
        text = result.get("text", "")
        score = result.get("score", 0.0)
        source_path = result.get("source_path", "N/A")
        
        context_parts.append(f"[D√°tum: {date}, Relevance: {score:.2f}]\n{text}")
        sources.append({
            "date": date,
            "score": score,
            "source_path": source_path
        })
    
    context = "\n\n---\n\n".join(context_parts)
    
    # 3. Synt√©za pomocou LLM cez OpenRouter
    api_key = load_api_key()
    if not api_key:
        return {
            "query": query,
            "error": "OPENROUTER_API_KEY nie je nastaven√Ω",
            "raw_results": rag_results
        }
    
    # V≈ΩDY pou≈æ√≠vame OpenRouter
    client = OpenAI(
        api_key=api_key, 
        base_url=OPENROUTER_BASE_URL,
        default_headers={
            "HTTP-Referer": "https://github.com/xvadur/system",
            "X-Title": "RAG Query Synthesis"
        }
    )
    
    # System prompt pre synt√©zu
    system_prompt = """Si expertn√Ω analytik a syntetiz√°tor inform√°ci√≠. Tvoja √∫loha je vytvori≈• syntetizovan√∫ odpoveƒè na z√°klade relevantn√Ωch textov z RAG vyhƒæad√°vania.

Po≈æiadavky:
- Odpoveƒè mus√≠ by≈• syntetizovan√° (nie len zoznam surov√Ωch textov)
- Obsahuj hlavn√© inform√°cie, kƒæ√∫ƒçov√© body a s√∫vislosti
- Zachovaj d√¥le≈æit√© fakty, d√°tumy a kontext
- Odpoveƒè mus√≠ by≈• ƒçitateƒæn√° a zmyslupln√°
- Ak je to chronol√≥gia, form√°tuj ju chronologicky
- Ak je to anal√Ωza, poskytni synt√©zu hlavn√Ωch my≈°lienok
- Pou≈æ√≠vaj markdown form√°tovanie pre prehƒæadnos≈•

NEPOU≈Ω√çVAJ:
- Zoznam surov√Ωch cit√°ci√≠
- Len prekop√≠rovan√© texty
- Neform√°tovan√Ω text

POU≈Ω√çVAJ:
- Syntetizovan√© odstavce
- Hlavn√© body a zhrnutia
- Chronologick√© usporiadanie (ak je relevantn√©)
- Markdown form√°tovanie"""
    
    user_prompt = f"""Ot√°zka: {query}

Relevantn√© texty z RAG vyhƒæad√°vania:
{context}

Vytvor syntetizovan√∫ odpoveƒè, ktor√° obsahuje hlavn√© inform√°cie z t√Ωchto textov. 
Odpoveƒè mus√≠ by≈• u≈æitoƒçn√° a ƒçitateƒæn√°, nie len zoznam surov√Ωch cit√°ci√≠."""
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=temperature
        )
        
        synthesized_answer = response.choices[0].message.content
        
        return {
            "query": query,
            "synthesized_answer": synthesized_answer,
            "sources_count": len(sources),
            "sources": sources,
            "raw_results_count": len(rag_results),
            "model_used": model
        }
        
    except Exception as e:
        return {
            "query": query,
            "error": f"Chyba pri synt√©ze: {str(e)}",
            "raw_results": rag_results
        }


def format_result(result: Dict) -> str:
    """Form√°tuje v√Ωsledok pre pekn√Ω v√Ωpis"""
    text = result.get("text", "")
    date = result.get("date", "N/A")
    source_path = result.get("source_path", "N/A")
    chunk_index = result.get("chunk_index", 0)
    total_chunks = result.get("total_chunks", 1)
    search_type = result.get("search_type", "semantic")
    content_type = result.get("content_type", "prompt")
    
    # Skr√°tenie textu ak je pr√≠li≈° dlh√Ω
    if len(text) > 500:
        text = text[:500] + "..."
    
    # Hybrid search info
    score_info = f"Score: {result['score']:.4f}"
    if search_type == "hybrid":
        semantic_score = result.get("semantic_score", 0)
        keyword_score = result.get("keyword_score", 0)
        score_info += f" (Semantic: {semantic_score:.4f}, Keyword: {keyword_score:.4f})"
    
    output = f"""
{'='*60}
Rank #{result['rank']} - {search_type.upper()} - {score_info}
Type: {content_type.upper()}
{'='*60}
D√°tum: {date}
Zdroj: {source_path}
Chunk: {chunk_index + 1}/{total_chunks}
{'='*60}

{text}

"""
    return output


def main():
    """Hlavn√° funkcia - volan√° z Cursor agenta alebo CLI"""
    if len(sys.argv) < 2:
        print("Pou≈æitie: python rag_agent_helper.py 'query' [top_k] [min_score] [use_hybrid] [mode] [content_type] [output_format]")
        print("\nParametre:")
        print("  query          - Vyhƒæad√°vac√≠ dotaz (povinn√Ω)")
        print("  top_k          - Poƒçet v√Ωsledkov (default: 5)")
        print("  min_score      - Minim√°lne similarity score (default: 0.4)")
        print("  use_hybrid     - Pou≈æi≈• hybrid search (default: true)")
        print("  mode           - 'search' alebo 'query' (default: search)")
        print("  content_type   - Filter: 'prompt', 'response', 'pair', alebo None (default: None)")
        print("  output_format  - 'json' alebo 'pretty' (default: json)")
        print("\nPr√≠klady:")
        print("  python rag_agent_helper.py 'ako som rie≈°il n8n' 5 0.4 true search None pretty")
        print("  python rag_agent_helper.py 'chronol√≥gia augusta' 10 0.4 true query None json")
        sys.exit(1)
    
    query = sys.argv[1]
    top_k = int(sys.argv[2]) if len(sys.argv) > 2 else 5
    min_score = float(sys.argv[3]) if len(sys.argv) > 3 else 0.4
    use_hybrid = sys.argv[4].lower() == "true" if len(sys.argv) > 4 else True
    mode = sys.argv[5].lower() if len(sys.argv) > 5 else "search"  # "search" alebo "query"
    content_type_filter = sys.argv[6] if len(sys.argv) > 6 else None
    output_format = sys.argv[7].lower() if len(sys.argv) > 7 else "json"  # "json" alebo "pretty"
    
    if content_type_filter and content_type_filter.lower() == "none":
        content_type_filter = None
    
    if mode == "query":
        # Syntetizovan√° odpoveƒè
        model = sys.argv[8] if len(sys.argv) > 8 else "deepseek/deepseek-v3.2"
        result = query_rag_with_synthesis(
            query=query,
            top_k=top_k,
            min_score=min_score,
            use_hybrid=use_hybrid,
            model=model
        )
        
        if output_format == "pretty":
            print("="*60)
            print("‚úÖ RAG QUERY - SYNTHESIS MODE")
            print("="*60)
            print(f"Dotaz: {query}")
            print(f"Model: {model}")
            print("="*60)
            print("\n" + result.get("synthesized_answer", "≈Ωiadna odpoveƒè"))
            print("\n" + "="*60)
            print(f"Zdrojov: {result.get('sources_count', 0)}")
            print("="*60)
        else:
            print(json.dumps(result, ensure_ascii=False, indent=2))
    else:
        # Search mode - surov√© v√Ωsledky
        results = search_rag(
            query, 
            top_k=top_k, 
            min_score=min_score, 
            use_hybrid=use_hybrid,
            content_type_filter=content_type_filter
        )
        
        if output_format == "pretty":
            print("="*60)
            print("üîç RAG SEARCH - " + ("HYBRID MODE" if use_hybrid else "SEMANTIC MODE"))
            print("="*60)
            print(f"Dotaz: {query}")
            print(f"Top K: {top_k}")
            print(f"Mode: {'Hybrid (Semantic + Keyword)' if use_hybrid else 'Semantic only'}")
            if content_type_filter:
                print(f"Content Type Filter: {content_type_filter}")
            print("="*60)
            print()
            
            if not results:
                print("‚ùå Nena≈°li sa ≈æiadne v√Ωsledky")
                return
            
            print(f"‚úÖ N√°jden√Ωch {len(results)} v√Ωsledkov:\n")
            
            for result in results:
                print(format_result(result))
        else:
            # JSON v√Ωstup (pre agenta)
            output = {
                "query": query,
                "search_type": "hybrid" if use_hybrid else "semantic",
                "results_count": len(results),
                "results": results
            }
            print(json.dumps(output, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()

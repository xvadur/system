{
  "id": "xNcEgqzlPqs",
  "title": "Why Your Al Agents Keep Failing (It's Not the Model)",
  "description": "My site: https://natebjones.com\nFull Story w/ Prompts: https://natesnewsletter.substack.com/p/executive-briefing-the-memory-gap?r=1z4sm5&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true\nMy substack: https://natesnewsletter.substack.com/\n_______________________\nWhat's really happening with AI agents and long-running memory? The common story is that smarter models solve agent failures — but the reality is more complicated.\n\nIn this video, I share the inside scoop on what Anthropic revealed about why agents actually work:\n\n • Why generalized agents behave like amnesiacs with tool belts\n • How domain memory turns chaotic loops into durable progress\n • What the initializer and coding agent pattern actually does\n • Where the real moat lies in harness design, not model intelligence\n\nFor builders and operators, the strategic insight is clear: the competitive advantage is not a smarter AI but well-designed domain memory and testing loops.\n\nSubscribe for daily AI strategy and news.\nFor deeper playbooks and analysis: https://natesnewsletter.substack.com/",
  "channel": "AI News & Strategy Daily | Nate B Jones",
  "channel_id": "UC0C-17n9iuUQPylguM1d-lQ",
  "duration": 816,
  "view_count": 513,
  "like_count": 41,
  "upload_date": "20251208",
  "thumbnail": "https://i.ytimg.com/vi/xNcEgqzlPqs/maxresdefault.jpg",
  "url": "https://www.youtube.com/watch?v=xNcEgqzlPqs",
  "extracted_at": "2025-12-08T22:45:39.195939"
}
"""
Log Manager - Triple-write syst√©m pre XVADUR logging s Hot/Cold Storage architekt√∫rou

Zapisuje s√∫ƒçasne do:
- XVADUR_LOG.md (Markdown pre ƒçloveka)
- XVADUR_LOG.jsonl (Hot Storage - posledn√Ωch 100 z√°znamov pre AI)
- archive.db (Cold Storage - SQLite arch√≠v pre historick√© query)

Automatick√© logovanie:
- Pri zad√°van√≠ tasku: log_task_started()
- Pri dokonƒçen√≠ tasku: log_task_completed()

Hot/Cold Storage architekt√∫ra:
- JSONL = Hot Storage (runtime kontext, posledn√Ωch 100 z√°znamov)
- SQLite = Cold Storage (arch√≠v, komplexn√© query, agreg√°cie)

Pou≈æitie:
    from scripts.utils.log_manager import add_log_entry, log_task_started, log_task_completed
    log_task_started("Implement√°cia feature X")
    # ... pr√°ca ...
    log_task_completed("Implement√°cia feature X", files_changed=["file.py"])
    
    # Historick√© query (Cold Storage)
    from scripts.utils.log_manager import query_archive, get_xp_summary
    results = query_archive(type="task", date_from="2025-12-01")
    summary = get_xp_summary()
"""

import sys
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Optional, Dict, Any

# Add workspace root to path
workspace_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(workspace_root))

# Context Engineering imports (lazy loading)
try:
    from core.context_engineering.token_metrics import TokenBudgetTracker, TokenBudget
    from core.context_engineering.config import (
        CONTEXT_WINDOW_SIZE,
        COMPRESSION_THRESHOLD,
        get_config
    )
    CONTEXT_ENGINEERING_AVAILABLE = True
except ImportError:
    CONTEXT_ENGINEERING_AVAILABLE = False
    CONTEXT_WINDOW_SIZE = 16000
    COMPRESSION_THRESHOLD = 0.8

# SQLite Store import (lazy loading)
try:
    from core.ministers.sqlite_store import SQLiteStore
    SQLITE_AVAILABLE = True
except ImportError:
    SQLITE_AVAILABLE = False

logger = logging.getLogger(__name__)

# === CESTY K S√öBOROM ===
LOG_MD_PATH = workspace_root / "development" / "logs" / "XVADUR_LOG.md"
LOG_JSONL_PATH = workspace_root / "development" / "logs" / "XVADUR_LOG.jsonl"
SQLITE_DB_PATH = workspace_root / "development" / "data" / "archive.db"

# === HOT STORAGE KONFIGUR√ÅCIA ===
HOT_STORAGE_LIMIT = 100  # Max z√°znamov v JSONL (Hot Storage)

# === SINGLETON PRE SQLITE STORE ===
_sqlite_store: Optional["SQLiteStore"] = None


def _get_sqlite_store() -> Optional["SQLiteStore"]:
    """Vr√°ti singleton in≈°tanciu SQLiteStore (lazy initialization)."""
    global _sqlite_store
    
    if not SQLITE_AVAILABLE:
        return None
    
    if _sqlite_store is None:
        try:
            _sqlite_store = SQLiteStore(SQLITE_DB_PATH)
            logger.debug(f"SQLiteStore inicializovan√Ω: {SQLITE_DB_PATH}")
        except Exception as e:
            logger.error(f"Chyba pri inicializ√°cii SQLiteStore: {e}")
            return None
    
    return _sqlite_store


def add_log_entry(
    action_name: str, 
    status: str, 
    files_changed: Optional[List[str]] = None, 
    xp_estimate: Optional[float] = None,
    entry_type: str = "task",
    completed: Optional[List[str]] = None,
    results: Optional[Dict[str, Any]] = None,
    decisions: Optional[List[str]] = None,
    quest_id: Optional[int] = None
):
    """Prid√° nov√Ω z√°znam do v≈°etk√Ωch storage vrstiev (triple-write).

    Zapisuje do:
    1. XVADUR_LOG.md (Markdown pre ƒçloveka)
    2. XVADUR_LOG.jsonl (Hot Storage - runtime kontext)
    3. archive.db (Cold Storage - SQLite arch√≠v)

    Args:
        action_name: N√°zov akcie (napr. "Implement√°cia session rotationu").
        status: Status akcie (napr. "started", "completed", "in_progress").
        files_changed: Zoznam zmienen√Ωch s√∫borov (voliteƒæn√©).
        xp_estimate: Odhad XP za dokonƒçenie akcie (voliteƒæn√©).
        entry_type: Typ z√°znamu ("task", "session", "quest_created", "quest_closed", "analysis", "savegame").
        completed: Zoznam dokonƒçen√Ωch polo≈æiek (voliteƒæn√©).
        results: Slovn√≠k s v√Ωsledkami (voliteƒæn√©).
        decisions: Zoznam kƒæ√∫ƒçov√Ωch rozhodnut√≠ (voliteƒæn√©).
        quest_id: ID GitHub Issue (voliteƒæn√©).
    """
    now = datetime.now()
    current_time = now.strftime('%H:%M')
    current_date = now.strftime('%Y-%m-%d')
    iso_timestamp = now.isoformat()

    # Vytvor entry dictionary (pou≈æit√© pre JSONL aj SQLite)
    entry = {
        "timestamp": iso_timestamp,
        "date": current_date,
        "time": current_time,
        "title": action_name,
        "type": entry_type,
        "status": status
    }
    
    # Pridaj voliteƒæn√© polia len ak existuj√∫
    if files_changed:
        entry["files_changed"] = files_changed
    if xp_estimate is not None:
        entry["xp_estimate"] = xp_estimate
    if completed:
        entry["completed"] = completed
    if results:
        entry["results"] = results
    if decisions:
        entry["decisions"] = decisions
    if quest_id is not None:
        entry["quest_id"] = quest_id

    # === 1. MARKDOWN Z√ÅZNAM ===
    _write_markdown_entry(action_name, status, files_changed, xp_estimate, current_time)

    # === 2. JSONL Z√ÅZNAM (Hot Storage) ===
    _write_jsonl_entry(entry)

    # === 3. SQLITE Z√ÅZNAM (Cold Storage) ===
    _write_sqlite_entry(entry)
    
    # === 4. AUTOMATICK√Å ARCHIV√ÅCIA ===
    _check_and_archive()


def _write_markdown_entry(
    action_name: str, 
    status: str, 
    files_changed: Optional[List[str]], 
    xp_estimate: Optional[float],
    current_time: str
):
    """Zap√≠≈°e z√°znam do Markdown logu."""
    log_entry = f"[{current_time}] üîπ {action_name}\n"
    if files_changed:
        log_entry += "  - *Zmenen√© s√∫bory:*\n"
        for f in files_changed:
            log_entry += f"    - {f}\n"
    log_entry += f"  - *Status:* {status}\n"
    if xp_estimate is not None:
        log_entry += f"  - *XP:* {xp_estimate}\n"

    try:
        # Naƒç√≠taj existuj√∫ci obsah
        if LOG_MD_PATH.exists():
            content = LOG_MD_PATH.read_text(encoding='utf-8')
        else:
            content = "# üß† XVADUR LOG\n\n**√öƒçel:** Z√°znam vykonanej pr√°ce a zmien v projekte\n\n---\n"
        
        # N√°jdi poz√≠ciu po hlaviƒçke (po prvom "---")
        if "---" in content:
            parts = content.split("---", 1)
            new_content = parts[0] + "---\n" + log_entry + "\n" + parts[1] if len(parts) > 1 else parts[0] + "---\n" + log_entry
        else:
            new_content = content + "\n" + log_entry
        
        LOG_MD_PATH.write_text(new_content, encoding='utf-8')
    except Exception as e:
        print(f"Error writing to XVADUR_LOG.md: {e}", file=sys.stderr)


def _write_jsonl_entry(entry: Dict[str, Any]):
    """Zap√≠≈°e z√°znam do JSONL logu (Hot Storage)."""
    try:
        # Append do JSONL s√∫boru
        with open(LOG_JSONL_PATH, 'a', encoding='utf-8') as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    except Exception as e:
        print(f"Error writing to XVADUR_LOG.jsonl: {e}", file=sys.stderr)


def _write_sqlite_entry(entry: Dict[str, Any]):
    """Zap√≠≈°e z√°znam do SQLite datab√°zy (Cold Storage)."""
    store = _get_sqlite_store()
    if store is None:
        logger.debug("SQLite nie je dostupn√Ω, preskakujem Cold Storage z√°pis")
        return
    
    try:
        store.insert(entry)
    except Exception as e:
        logger.error(f"Chyba pri z√°pise do SQLite: {e}")


def _count_jsonl_entries() -> int:
    """Spoƒç√≠ta poƒçet z√°znamov v JSONL s√∫bore."""
    if not LOG_JSONL_PATH.exists():
        return 0
    
    try:
        with open(LOG_JSONL_PATH, 'r', encoding='utf-8') as f:
            return sum(1 for line in f if line.strip())
    except Exception as e:
        logger.error(f"Chyba pri poƒç√≠tan√≠ JSONL z√°znamov: {e}")
        return 0


def _check_and_archive():
    """Skontroluje ƒçi je potrebn√° archiv√°cia a vykon√° ju."""
    count = _count_jsonl_entries()
    
    if count > HOT_STORAGE_LIMIT:
        logger.info(f"JSONL m√° {count} z√°znamov (limit: {HOT_STORAGE_LIMIT}), sp√∫≈°≈•am archiv√°ciu")
        archive_old_entries()


def archive_old_entries():
    """Archivuje star√© z√°znamy - ponech√° len posledn√Ωch HOT_STORAGE_LIMIT v JSONL.
    
    T√°to funkcia:
    1. Naƒç√≠ta v≈°etky z√°znamy z JSONL
    2. Ponech√° posledn√Ωch HOT_STORAGE_LIMIT z√°znamov
    3. Prep√≠≈°e JSONL len s t√Ωmito z√°znamami
    
    Pozn√°mka: Z√°znamy s√∫ u≈æ v SQLite (Cold Storage), tak≈æe sa nestr√°caj√∫.
    """
    if not LOG_JSONL_PATH.exists():
        return
    
    try:
        # Naƒç√≠taj v≈°etky z√°znamy
        entries = []
        with open(LOG_JSONL_PATH, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    entries.append(json.loads(line))
        
        if len(entries) <= HOT_STORAGE_LIMIT:
            logger.debug(f"Archiv√°cia nie je potrebn√° ({len(entries)} <= {HOT_STORAGE_LIMIT})")
            return
        
        # Ponechaj len posledn√Ωch HOT_STORAGE_LIMIT
        keep_entries = entries[-HOT_STORAGE_LIMIT:]
        archived_count = len(entries) - len(keep_entries)
        
        # Prep√≠≈° JSONL
        with open(LOG_JSONL_PATH, 'w', encoding='utf-8') as f:
            for entry in keep_entries:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        
        logger.info(f"Archivovan√© {archived_count} z√°znamov, ponechan√Ωch {len(keep_entries)} v Hot Storage")
        
    except Exception as e:
        logger.error(f"Chyba pri archiv√°cii: {e}")


def get_recent_log_entries(limit: int = 5) -> List[Dict[str, Any]]:
    """Naƒç√≠ta posledn√Ωch N z√°znamov z JSONL logu (Hot Storage).
    
    Args:
        limit: Poƒçet z√°znamov na naƒç√≠tanie (default: 5)
    
    Returns:
        Zoznam posledn√Ωch N log z√°znamov
    """
    if not LOG_JSONL_PATH.exists():
        return []
    
    entries = []
    try:
        with open(LOG_JSONL_PATH, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    entries.append(json.loads(line))
        return entries[-limit:]
    except Exception as e:
        print(f"Error reading XVADUR_LOG.jsonl: {e}", file=sys.stderr)
        return []


def log_task_started(task_name: str, task_description: Optional[str] = None) -> None:
    """Automaticky zaloguje zaƒçiatok tasku.
    
    Args:
        task_name: N√°zov tasku
        task_description: Voliteƒæn√Ω popis tasku
    """
    add_log_entry(
        action_name=f"Task: {task_name}",
        status="started",
        entry_type="task",
        results={"description": task_description} if task_description else None
    )
    logger.info(f"Task started: {task_name}")


def log_task_completed(
    task_name: str,
    files_changed: Optional[List[str]] = None,
    xp_estimate: Optional[float] = None,
    completed: Optional[List[str]] = None,
    results: Optional[Dict[str, Any]] = None,
    decisions: Optional[List[str]] = None
) -> None:
    """Automaticky zaloguje dokonƒçenie tasku.
    
    Args:
        task_name: N√°zov tasku
        files_changed: Zoznam zmienen√Ωch s√∫borov
        xp_estimate: Odhad XP
        completed: Zoznam dokonƒçen√Ωch polo≈æiek
        results: V√Ωsledky tasku
        decisions: Kƒæ√∫ƒçov√© rozhodnutia
    """
    # Trackuj tokeny ak je Context Engineering dostupn√Ω
    token_metrics = None
    if CONTEXT_ENGINEERING_AVAILABLE:
        try:
            tracker = TokenBudgetTracker(TokenBudget(context_window_size=CONTEXT_WINDOW_SIZE))
            # Odhad tokenov pre tento z√°znam
            entry_text = f"{task_name} {json.dumps(results or {})}"
            token_count = tracker.estimate_tokens(entry_text)
            token_metrics = {
                "token_count": token_count,
                "context_window_size": CONTEXT_WINDOW_SIZE
            }
        except Exception as e:
            logger.warning(f"Chyba pri trackovan√≠ tokenov: {e}")
    
    # Pridaj token metriky do results
    if results is None:
        results = {}
    if token_metrics:
        results["token_metrics"] = token_metrics
    
    add_log_entry(
        action_name=f"Task: {task_name}",
        status="completed",
        files_changed=files_changed,
        xp_estimate=xp_estimate,
        entry_type="task",
        completed=completed,
        results=results,
        decisions=decisions
    )
    logger.info(f"Task completed: {task_name}")


def get_optimized_log_context(limit: int = 5, use_compression: bool = False) -> Dict[str, Any]:
    """Naƒç√≠ta optimalizovan√Ω kontext z logu pomocou Context Engineering.
    
    Pou≈æ√≠va Hot Storage (JSONL) pre r√Ωchle naƒç√≠tanie.
    
    Args:
        limit: Poƒçet z√°znamov na naƒç√≠tanie
        use_compression: Pou≈æi≈• kompresiu ak je utilization vysok√°
    
    Returns:
        Dict s optimalizovan√Ωm kontextom a metrikami
    """
    entries = get_recent_log_entries(limit=limit * 2)  # Naƒç√≠taj viac pre optimaliz√°ciu
    
    if not entries:
        return {"entries": [], "token_metrics": None, "optimized": False}
    
    # Konvertuj na text pre token tracking
    entries_text = "\n".join([json.dumps(e, ensure_ascii=False) for e in entries])
    
    if CONTEXT_ENGINEERING_AVAILABLE:
        try:
            tracker = TokenBudgetTracker(TokenBudget(context_window_size=CONTEXT_WINDOW_SIZE))
            metrics = tracker.track_usage(history_content=entries_text)
            utilization = metrics.utilization_ratio(CONTEXT_WINDOW_SIZE)
            
            # Ak je utilization vysok√° a je po≈æadovan√° kompresia, aplikuj ju
            optimized_entries = entries
            if use_compression and utilization > COMPRESSION_THRESHOLD:
                # Zjednodu≈°en√° kompresia - vezmi len najnov≈°ie a najd√¥le≈æitej≈°ie
                optimized_entries = entries[-limit:]
                logger.info(f"Kompresia logu: {len(entries)} -> {len(optimized_entries)} z√°znamov")
            
            return {
                "entries": optimized_entries[-limit:],
                "token_metrics": metrics.to_dict(),
                "utilization": utilization,
                "optimized": len(optimized_entries) < len(entries)
            }
        except Exception as e:
            logger.warning(f"Chyba pri optimaliz√°cii logu: {e}")
    
    return {
        "entries": entries[-limit:],
        "token_metrics": None,
        "optimized": False
    }


# =============================================================================
# COLD STORAGE API (SQLite Query Functions)
# =============================================================================

def query_archive(
    type: Optional[str] = None,
    status: Optional[str] = None,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None,
    quest_id: Optional[int] = None,
    limit: int = 100
) -> List[Dict[str, Any]]:
    """Vyhƒæad√° z√°znamy v Cold Storage (SQLite arch√≠v).
    
    Args:
        type: Filter podƒæa typu (task, session, quest_created, atƒè.)
        status: Filter podƒæa statusu (started, completed, atƒè.)
        date_from: Filter od d√°tumu (YYYY-MM-DD)
        date_to: Filter do d√°tumu (YYYY-MM-DD)
        quest_id: Filter podƒæa quest ID
        limit: Maxim√°lny poƒçet v√Ωsledkov
        
    Returns:
        Zoznam z√°znamov ako dictionary
    """
    store = _get_sqlite_store()
    if store is None:
        logger.warning("SQLite nie je dostupn√Ω, query_archive vracia pr√°zdny zoznam")
        return []
    
    try:
        return store.query(
            type=type,
            status=status,
            date_from=date_from,
            date_to=date_to,
            quest_id=quest_id,
            limit=limit
        )
    except Exception as e:
        logger.error(f"Chyba pri query_archive: {e}")
        return []


def get_archive_count(
    type: Optional[str] = None,
    status: Optional[str] = None,
    quest_id: Optional[int] = None
) -> int:
    """Spoƒç√≠ta z√°znamy v Cold Storage.
    
    Args:
        type: Filter podƒæa typu
        status: Filter podƒæa statusu
        quest_id: Filter podƒæa quest ID
        
    Returns:
        Poƒçet z√°znamov
    """
    store = _get_sqlite_store()
    if store is None:
        return 0
    
    try:
        return store.count(type=type, status=status, quest_id=quest_id)
    except Exception as e:
        logger.error(f"Chyba pri get_archive_count: {e}")
        return 0


def get_xp_summary() -> Dict[str, Any]:
    """Vr√°ti sum√°r XP zo v≈°etk√Ωch z√°znamov v Cold Storage.
    
    Returns:
        Dictionary s XP ≈°tatistikami:
        - total_xp: Celkov√© XP
        - by_type: XP rozdelen√© podƒæa typu
        - by_day: XP rozdelen√© podƒæa d≈àa (posledn√Ωch 7 dn√≠)
    """
    store = _get_sqlite_store()
    if store is None:
        return {"total_xp": 0, "by_type": [], "by_day": []}
    
    try:
        return store.get_xp_summary()
    except Exception as e:
        logger.error(f"Chyba pri get_xp_summary: {e}")
        return {"total_xp": 0, "by_type": [], "by_day": []}


def get_quest_summary(quest_id: int) -> Dict[str, Any]:
    """Vr√°ti sum√°r pre konkr√©tny quest z Cold Storage.
    
    Args:
        quest_id: ID questu
        
    Returns:
        Dictionary so sum√°rom questu
    """
    store = _get_sqlite_store()
    if store is None:
        return {"quest_id": quest_id, "found": False}
    
    try:
        return store.get_quest_summary(quest_id)
    except Exception as e:
        logger.error(f"Chyba pri get_quest_summary: {e}")
        return {"quest_id": quest_id, "found": False}


def aggregate_xp(group_by: str = "type") -> List[Dict[str, Any]]:
    """Agreguje XP podƒæa zadan√©ho poƒæa.
    
    Args:
        group_by: Pole pre GROUP BY (type, status, date, quest_id)
        
    Returns:
        Zoznam agregovan√Ωch v√Ωsledkov
    """
    store = _get_sqlite_store()
    if store is None:
        return []
    
    try:
        return store.aggregate("xp_estimate", agg_func="SUM", group_by=group_by)
    except Exception as e:
        logger.error(f"Chyba pri aggregate_xp: {e}")
        return []


def get_storage_stats() -> Dict[str, Any]:
    """Vr√°ti ≈°tatistiky o Hot a Cold Storage.
    
    Returns:
        Dictionary so ≈°tatistikami:
        - hot_storage_count: Poƒçet z√°znamov v JSONL
        - cold_storage_count: Poƒçet z√°znamov v SQLite
        - hot_storage_limit: Limit pre Hot Storage
        - sqlite_available: ƒåi je SQLite dostupn√Ω
    """
    hot_count = _count_jsonl_entries()
    
    store = _get_sqlite_store()
    cold_count = store.count() if store else 0
    
    return {
        "hot_storage_count": hot_count,
        "cold_storage_count": cold_count,
        "hot_storage_limit": HOT_STORAGE_LIMIT,
        "sqlite_available": SQLITE_AVAILABLE and store is not None
    }


if __name__ == "__main__":
    # Pr√≠klad pou≈æitia triple-write
    print("=" * 60)
    print("LOG MANAGER - HOT/COLD STORAGE TEST")
    print("=" * 60)
    
    # Test triple-write
    add_log_entry(
        action_name="Test Hot/Cold Storage syst√©mu",
        status="completed",
        files_changed=["scripts/utils/log_manager.py"],
        xp_estimate=5.0,
        entry_type="task",
        completed=["Implement√°cia triple-write", "Pridanie SQLite podpory"],
        results={"md_write": "OK", "jsonl_write": "OK", "sqlite_write": "OK"}
    )
    print("‚úÖ Triple-write test: Z√°znam pridan√Ω do MD, JSONL aj SQLite")
    
    # Uk√°≈æ storage stats
    stats = get_storage_stats()
    print(f"\nüìä Storage Stats:")
    print(f"   - Hot Storage (JSONL): {stats['hot_storage_count']} z√°znamov")
    print(f"   - Cold Storage (SQLite): {stats['cold_storage_count']} z√°znamov")
    print(f"   - Hot Storage Limit: {stats['hot_storage_limit']}")
    print(f"   - SQLite Available: {stats['sqlite_available']}")
    
    # Uk√°≈æ posledn√© z√°znamy z Hot Storage
    recent = get_recent_log_entries(3)
    print(f"\nüìã Posledn√© 3 z√°znamy z Hot Storage (JSONL):")
    for entry in recent:
        print(f"   - [{entry.get('time')}] {entry.get('title')}")
    
    # Uk√°≈æ XP summary z Cold Storage
    if stats['sqlite_available']:
        xp_summary = get_xp_summary()
        print(f"\nüí∞ XP Summary z Cold Storage (SQLite):")
        print(f"   - Total XP: {xp_summary.get('total_xp', 0)}")
        print(f"   - By Type: {xp_summary.get('by_type', [])}")

#!/usr/bin/env python3
"""
OdstrÃ¡nenie duplikÃ¡tov z vyÄistenÃ½ch dÃ¡t na zÃ¡klade analÃ½zy duplikÃ¡tov.
PonechÃ¡ len prvÃ½ vÃ½skyt kaÅ¾dÃ©ho duplikÃ¡tu.
"""

import json
import hashlib
from pathlib import Path
from typing import Dict, Set
from collections import defaultdict

workspace_root = Path(__file__).parent.parent
input_dir = workspace_root / "xvadur" / "data" / "kortex_cleaned"
analysis_file = workspace_root / "xvadur" / "data" / "kortex_analysis" / "duplicate_text_blocks.json"
output_dir = workspace_root / "xvadur" / "data" / "kortex_final"

output_dir.mkdir(parents=True, exist_ok=True)

print("ğŸ—‘ï¸  OdstrÃ¡nenie duplikÃ¡tov z vyÄistenÃ½ch dÃ¡t\n")
print(f"ğŸ“ Input: {input_dir}")
print(f"ğŸ“ Output: {output_dir}\n")

# KonfigurÃ¡cia
MIN_BLOCK_SIZE = 500
HASH_SAMPLE_SIZE = 1000


def normalize_text(text: str) -> str:
    """Normalizuje text pre hash."""
    return " ".join(text.lower().split())


def load_duplicate_hashes(analysis_file: Path) -> Dict[str, Set[str]]:
    """NaÄÃ­ta hash duplikÃ¡tov z analÃ½zy."""
    if not analysis_file.exists():
        print(f"  âš ï¸  AnalÃ½za neexistuje: {analysis_file}")
        return {"user_prompts": set(), "ai_responses": set()}
    
    print(f"  ğŸ“– NaÄÃ­tavam analÃ½zu duplikÃ¡tov...")
    
    with open(analysis_file, 'r', encoding='utf-8') as f:
        analysis_data = json.load(f)
    
    duplicate_hashes = {
        "user_prompts": set(),
        "ai_responses": set(),
    }
    
    # ZÃ­skame hash duplikÃ¡tov z analÃ½zy
    # (potrebujeme rekonÅ¡truovaÅ¥ hash z textovÃ½ch vzoriek)
    # Alebo pouÅ¾ijeme inÃ½ prÃ­stup - preÄÃ­tame vÅ¡etky duplikÃ¡ty a vytvorÃ­me hash mapu
    
    return duplicate_hashes


def remove_duplicates(input_file: Path, output_file: Path, file_type: str) -> Dict:
    """OdstrÃ¡ni duplikÃ¡ty z jednÃ©ho sÃºboru."""
    print(f"\nğŸ“„ ÄŒistÃ­m {file_type}...")
    
    if not input_file.exists():
        print(f"  âš ï¸  SÃºbor neexistuje: {input_file}")
        return {}
    
    seen_hashes: Dict[str, int] = {}  # hash -> line number prvÃ©ho vÃ½skytu
    duplicates_removed = 0
    total_count = 0
    kept_count = 0
    
    print(f"  ğŸ“– NaÄÃ­tavam sÃºbor a identifikujem duplikÃ¡ty...")
    
    # Najprv prejdeme sÃºbor a identifikujeme duplikÃ¡ty
    with open(input_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                data = json.loads(line)
                text = data.get("extracted_text", "")
                total_count += 1
                
                if not text or len(text) < MIN_BLOCK_SIZE:
                    # KrÃ¡tke texty ponechÃ¡me
                    continue
                
                # Normalizujeme text
                normalized = normalize_text(text)
                hash_sample = normalized[:HASH_SAMPLE_SIZE]
                text_hash = hashlib.md5(hash_sample.encode()).hexdigest()
                
                # Ak sme uÅ¾ videli tento hash, je to duplikÃ¡t
                if text_hash in seen_hashes:
                    duplicates_removed += 1
                    continue
                
                # PrvÃ½ vÃ½skyt - uloÅ¾Ã­me hash
                seen_hashes[text_hash] = line_num
                
            except Exception:
                continue
    
    print(f"  âœ… NÃ¡jdenÃ½ch {duplicates_removed} duplikÃ¡tov z {total_count} textov")
    
    # Teraz prejdeme sÃºbor znova a zapÃ­Å¡eme len unikÃ¡tne zÃ¡znamy
    print(f"  ğŸ’¾ UkladÃ¡m vyÄistenÃ½ sÃºbor...")
    
    seen_hashes = {}  # Resetujeme
    duplicates_removed = 0
    kept_count = 0
    
    with open(input_file, 'r', encoding='utf-8') as f_in, \
         open(output_file, 'w', encoding='utf-8') as f_out:
        
        for line_num, line in enumerate(f_in, 1):
            try:
                data = json.loads(line)
                text = data.get("extracted_text", "")
                
                # KrÃ¡tke texty vÅ¾dy ponechÃ¡me
                if not text or len(text) < MIN_BLOCK_SIZE:
                    f_out.write(line)
                    kept_count += 1
                    continue
                
                # Normalizujeme text
                normalized = normalize_text(text)
                hash_sample = normalized[:HASH_SAMPLE_SIZE]
                text_hash = hashlib.md5(hash_sample.encode()).hexdigest()
                
                # Ak sme uÅ¾ videli tento hash, preskoÄÃ­me (duplikÃ¡t)
                if text_hash in seen_hashes:
                    duplicates_removed += 1
                    continue
                
                # PrvÃ½ vÃ½skyt - uloÅ¾Ã­me a zapÃ­Å¡eme
                seen_hashes[text_hash] = line_num
                f_out.write(line)
                kept_count += 1
                
            except Exception:
                continue
    
    print(f"  âœ… OdstrÃ¡nenÃ½ch {duplicates_removed} duplikÃ¡tov")
    print(f"  âœ… PonechanÃ½ch {kept_count} unikÃ¡tnych textov")
    print(f"  âœ… UloÅ¾enÃ©: {output_file.name}\n")
    
    return {
        "total": total_count,
        "kept": kept_count,
        "removed": duplicates_removed,
    }


def remove_duplicates_from_pairs(input_file: Path, output_file: Path, user_hashes: Set[str], ai_hashes: Set[str]) -> Dict:
    """OdstrÃ¡ni duplikÃ¡ty z konverzaÄnÃ½ch pÃ¡rov."""
    print(f"\nğŸ”— ÄŒistÃ­m konverzaÄnÃ© pÃ¡ry...")
    
    if not input_file.exists():
        print(f"  âš ï¸  SÃºbor neexistuje: {input_file}")
        return {}
    
    seen_pair_hashes: Dict[str, int] = {}
    duplicates_removed = 0
    total_count = 0
    kept_count = 0
    
    print(f"  ğŸ“– NaÄÃ­tavam sÃºbor...")
    
    with open(input_file, 'r', encoding='utf-8') as f_in, \
         open(output_file, 'w', encoding='utf-8') as f_out:
        
        for line_num, line in enumerate(f_in, 1):
            try:
                data = json.loads(line)
                total_count += 1
                
                user_prompt = data.get("user_prompt", {})
                ai_response = data.get("ai_response", {})
                
                user_text = user_prompt.get("extracted_text", "")
                ai_text = ai_response.get("extracted_text", "")
                
                if not user_text or not ai_text:
                    continue
                
                # Skontrolujeme, Äi user prompt alebo AI odpoveÄ nie sÃº duplikÃ¡ty
                user_normalized = normalize_text(user_text)
                ai_normalized = normalize_text(ai_text)
                
                user_hash = hashlib.md5(user_normalized[:HASH_SAMPLE_SIZE].encode()).hexdigest()
                ai_hash = hashlib.md5(ai_normalized[:HASH_SAMPLE_SIZE].encode()).hexdigest()
                
                # Ak je user prompt alebo AI odpoveÄ duplikÃ¡t, preskoÄÃ­me
                if user_hash not in user_hashes and len(user_text) >= MIN_BLOCK_SIZE:
                    # User prompt je duplikÃ¡t (nie je v cleaned user prompts)
                    duplicates_removed += 1
                    continue
                
                if ai_hash not in ai_hashes and len(ai_text) >= MIN_BLOCK_SIZE:
                    # AI odpoveÄ je duplikÃ¡t
                    duplicates_removed += 1
                    continue
                
                # Kontrola duplikÃ¡tu pÃ¡ru
                pair_hash = hashlib.md5((user_hash + ai_hash).encode()).hexdigest()
                if pair_hash in seen_pair_hashes:
                    duplicates_removed += 1
                    continue
                
                seen_pair_hashes[pair_hash] = line_num
                f_out.write(line)
                kept_count += 1
                
            except Exception:
                continue
    
    print(f"  âœ… OdstrÃ¡nenÃ½ch {duplicates_removed} duplikÃ¡tov")
    print(f"  âœ… PonechanÃ½ch {kept_count} unikÃ¡tnych pÃ¡rov")
    print(f"  âœ… UloÅ¾enÃ©: {output_file.name}\n")
    
    return {
        "total": total_count,
        "kept": kept_count,
        "removed": duplicates_removed,
    }


def main():
    """HlavnÃ¡ funkcia."""
    
    results = {}
    
    print("=" * 60)
    
    # Najprv vyÄistÃ­me user prompty a AI odpovede
    user_stats = remove_duplicates(
        input_dir / "user_prompts_cleaned.jsonl",
        output_dir / "user_prompts_final.jsonl",
        "user prompty"
    )
    results["user_prompts"] = user_stats
    
    ai_stats = remove_duplicates(
        input_dir / "ai_responses_cleaned.jsonl",
        output_dir / "ai_responses_final.jsonl",
        "AI odpovede"
    )
    results["ai_responses"] = ai_stats
    
    # NaÄÃ­tame hash mapy vyÄistenÃ½ch user promptov a AI odpovedÃ­
    print(f"\nğŸ“– NaÄÃ­tavam hash mapy vyÄistenÃ½ch dÃ¡t...")
    
    user_hashes = set()
    with open(output_dir / "user_prompts_final.jsonl", 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            text = data.get("extracted_text", "")
            if text and len(text) >= MIN_BLOCK_SIZE:
                normalized = normalize_text(text)
                hash_val = hashlib.md5(normalized[:HASH_SAMPLE_SIZE].encode()).hexdigest()
                user_hashes.add(hash_val)
    
    ai_hashes = set()
    with open(output_dir / "ai_responses_final.jsonl", 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            text = data.get("extracted_text", "")
            if text and len(text) >= MIN_BLOCK_SIZE:
                normalized = normalize_text(text)
                hash_val = hashlib.md5(normalized[:HASH_SAMPLE_SIZE].encode()).hexdigest()
                ai_hashes.add(hash_val)
    
    print(f"  âœ… NaÄÃ­tanÃ½ch {len(user_hashes)} user prompt hashov")
    print(f"  âœ… NaÄÃ­tanÃ½ch {len(ai_hashes)} AI odpoveÄ hashov")
    
    # Teraz vyÄistÃ­me konverzaÄnÃ© pÃ¡ry
    pairs_stats = remove_duplicates_from_pairs(
        input_dir / "conversation_pairs_cleaned.jsonl",
        output_dir / "conversation_pairs_final.jsonl",
        user_hashes,
        ai_hashes
    )
    results["conversation_pairs"] = pairs_stats
    
    # UloÅ¾Ã­me Å¡tatistiky
    stats_file = output_dir / "removal_stats.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # SÃºhrn
    print("=" * 60)
    print("ğŸ“Š SÃšHRN ODSTÃNENIA DUPLIKÃTOV")
    print("=" * 60)
    
    print(f"\nğŸ“ User prompty:")
    print(f"  PonechanÃ½ch: {user_stats.get('kept', 0)}")
    print(f"  OdstrÃ¡nenÃ½ch: {user_stats.get('removed', 0)}")
    
    print(f"\nğŸ¤– AI odpovede:")
    print(f"  PonechanÃ½ch: {ai_stats.get('kept', 0)}")
    print(f"  OdstrÃ¡nenÃ½ch: {ai_stats.get('removed', 0)}")
    
    print(f"\nğŸ”— KonverzaÄnÃ© pÃ¡ry:")
    print(f"  PonechanÃ½ch: {pairs_stats.get('kept', 0)}")
    print(f"  OdstrÃ¡nenÃ½ch: {pairs_stats.get('removed', 0)}")
    
    print(f"\nğŸ’¾ Å tatistiky uloÅ¾enÃ©: {stats_file}")
    print(f"\nğŸ‰ OdstrÃ¡nenie duplikÃ¡tov dokonÄenÃ©!")
    print(f"ğŸ“ FinÃ¡lne sÃºbory: {output_dir}")


if __name__ == "__main__":
    main()


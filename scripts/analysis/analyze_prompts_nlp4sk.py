#!/usr/bin/env python3
"""
Analyzuje prompty pomocou lok√°lnych NLP n√°strojov (Stanza, Hugging Face, spaCy).
Extrahuje ent√≠t, sentiment a pojmy z ka≈æd√©ho promptu.
"""

import json
import os
import sys
import time
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
from collections import defaultdict

# Lok√°lne NLP kni≈ænice
try:
    import stanza
    from transformers import pipeline
except ImportError:
    print("‚ùå Chyba: Potrebuje≈° nain≈°talova≈• stanza a transformers")
    print("   pip install stanza transformers torch")
    sys.exit(1)

# Konfigur√°cia
PROMPTS_SPLIT_DIR = Path("data/prompts/prompts_split")
PROMPTS_LOG_PATH = Path("xvadur/data/prompts_log.jsonl")
OUTPUT_FILE = Path("data/prompts/prompts_nlp4sk.jsonl")
ERROR_LOG = Path("data/prompts/nlp4sk_errors.log")
BATCH_SIZE = 10
TEST_MODE = False  # Ak True, spracuje len prv√Ωch 20 promptov
TEST_LIMIT = 20
MAX_WORDS = 1000  # Filtrovanie dlh√Ωch promptov

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(ERROR_LOG),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Glob√°lne pipeline objekty (lazy loading)
_stanza_nlp = None
_sentiment_pipeline = None


def init_stanza():
    """Inicializuje Stanza pipeline pre slovenƒçinu."""
    global _stanza_nlp
    
    if _stanza_nlp is not None:
        return _stanza_nlp
    
    try:
        logger.info("üîÑ Inicializujem Stanza pipeline pre slovenƒçinu...")
        logger.info("   (Prv√© spustenie m√¥≈æe trva≈• ~1-2 min√∫ty - stiahnutie modelu)")
        # Pou≈æijeme default package, ktor√Ω obsahuje v≈°etky procesory vr√°tane NER
        _stanza_nlp = stanza.Pipeline('sk', use_gpu=False)
        logger.info("‚úÖ Stanza pipeline inicializovan√Ω")
        return _stanza_nlp
    except Exception as e:
        logger.error(f"‚ùå Chyba pri inicializ√°cii Stanza: {e}")
        logger.error("   Sk√∫s: python3 -c 'import stanza; stanza.download(\"sk\")'")
        # Fallback - sk√∫s bez NER
        try:
            logger.info("üîÑ Sk√∫≈°am bez NER...")
            _stanza_nlp = stanza.Pipeline('sk', processors='tokenize,lemma,pos', use_gpu=False)
            logger.info("‚úÖ Stanza pipeline inicializovan√Ω (bez NER)")
            return _stanza_nlp
        except Exception as e2:
            logger.error(f"‚ùå Chyba aj bez NER: {e2}")
            return None


def init_sentiment_pipeline():
    """Inicializuje Hugging Face sentiment pipeline."""
    global _sentiment_pipeline
    
    if _sentiment_pipeline is not None:
        return _sentiment_pipeline
    
    try:
        logger.info("üîÑ Inicializujem Hugging Face sentiment pipeline...")
        logger.info("   (Prv√© spustenie m√¥≈æe trva≈• ~2-3 min√∫ty - stiahnutie modelu)")
        import torch
        device = -1 if not torch.cuda.is_available() else 0
        
        # Sk√∫s najprv multilingual model optimalizovan√Ω pre v≈°eobecn√Ω text
        try:
            logger.info("   Sk√∫≈°am nlptown/bert-base-multilingual-uncased-sentiment...")
            _sentiment_pipeline = pipeline(
                "sentiment-analysis",
                model="nlptown/bert-base-multilingual-uncased-sentiment",
                device=device
            )
            logger.info("‚úÖ Sentiment pipeline inicializovan√Ω (multilingual BERT)")
            return _sentiment_pipeline
        except Exception as e1:
            logger.warning(f"   Prv√Ω model zlyhal: {e1}")
            # Fallback - sk√∫s default model
            try:
                logger.info("   Sk√∫≈°am default sentiment model...")
                _sentiment_pipeline = pipeline("sentiment-analysis", device=device)
                logger.info("‚úÖ Alternat√≠vny sentiment pipeline inicializovan√Ω (default)")
                return _sentiment_pipeline
            except Exception as e2:
                logger.error(f"‚ùå Chyba aj s default modelom: {e2}")
                return None
    except Exception as e:
        logger.error(f"‚ùå Chyba pri inicializ√°cii sentiment pipeline: {e}")
        import traceback
        logger.debug(traceback.format_exc())
        return None


def extract_sentiment(text: str, sentiment_pipeline) -> Optional[Dict]:
    """Extrahuje sentiment pomocou Hugging Face transformers."""
    if not sentiment_pipeline:
        return None
    
    try:
        # Limit na 512 znakov (transformers limit)
        text_to_analyze = text[:512]
        result = sentiment_pipeline(text_to_analyze)
        
        if not result or len(result) == 0:
            return None
        
        # R√¥zne modely m√¥≈æu vraca≈• r√¥zne form√°ty
        if isinstance(result, list):
            first_result = result[0] if len(result) > 0 else result
        else:
            first_result = result
        
        label = str(first_result.get('label', '')).upper()
        score = float(first_result.get('score', 0.0))
        
        # Mapovanie pre r√¥zne modely
        # nlptown/bert-base-multilingual-uncased-sentiment pou≈æ√≠va: "1 star", "2 stars", "3 stars", "4 stars", "5 stars"
        # Default model pou≈æ√≠va: POSITIVE, NEGATIVE
        # Twitter model pou≈æ√≠va: LABEL_0 (negative), LABEL_1 (neutral), LABEL_2 (positive)
        
        sentiment_map = {
            # Twitter model
            'LABEL_0': 'negative',
            'LABEL_1': 'neutral', 
            'LABEL_2': 'positive',
            # Default model
            'NEGATIVE': 'negative',
            'POSITIVE': 'positive',
            # nlptown model (5-star rating) - case insensitive
            '1 STAR': 'negative',
            '1 STARS': 'negative',
            '2 STAR': 'negative',
            '2 STARS': 'negative',
            '3 STAR': 'neutral',
            '3 STARS': 'neutral',
            '4 STAR': 'positive',
            '4 STARS': 'positive',
            '5 STAR': 'positive',
            '5 STARS': 'positive',
            # S lowercase (ak by sa stalo)
            '1 star': 'negative',
            '1 stars': 'negative',
            '2 star': 'negative',
            '2 stars': 'negative',
            '3 star': 'neutral',
            '3 stars': 'neutral',
            '4 star': 'positive',
            '4 stars': 'positive',
            '5 star': 'positive',
            '5 stars': 'positive',
            # Numerick√© verzie
            '1': 'negative',
            '2': 'negative',
            '3': 'neutral',
            '4': 'positive',
            '5': 'positive',
        }
        
        # Normalizuj label (odstr√°ni medzery, lowercase)
        label_normalized = label.strip().upper()
        sentiment = sentiment_map.get(label_normalized, 'neutral')
        
        # Pre nlptown model: ak je 3 stars, pova≈æuj za neutral
        # Ak je score n√≠zky (< 0.4), pova≈æuj za neutral (menej agres√≠vne)
        if score < 0.4:
            sentiment = 'neutral'
        
        return {
            "sentiment": sentiment,
            "sentiment_score": score,
            "raw": result,
            "raw_label": label  # Pre debugging
        }
    except Exception as e:
        logger.warning(f"Chyba pri sentiment anal√Ωze: {e}")
        import traceback
        logger.debug(traceback.format_exc())
        return None


def extract_entities(text: str, stanza_nlp) -> Optional[Dict]:
    """Extrahuje ent√≠t pomocou Stanza NER alebo alternat√≠vnych met√≥d."""
    if not stanza_nlp:
        return None
    
    try:
        doc = stanza_nlp(text)
        
        people = []
        organizations = []
        locations = []
        technologies = []
        
        # Tech keywords pre identifik√°ciu technol√≥gi√≠
        tech_keywords = ["api", "python", "javascript", "react", "openai", "n8n", 
                        "chainlit", "mcp", "docker", "git", "github", "node", "npm",
                        "typescript", "vue", "angular", "fastapi", "flask", "django",
                        "postgresql", "mysql", "mongodb", "redis", "elasticsearch"]
        
        # Sk√∫s pou≈æi≈• NER ak je dostupn√Ω
        has_ner = False
        for sentence in doc.sentences:
            if hasattr(sentence, 'entities') and sentence.entities:
                has_ner = True
                for entity in sentence.entities:
                    entity_type = entity.type
                    entity_text = entity.text
                    
                    if entity_type == "PERSON":
                        people.append(entity_text)
                    elif entity_type in ["ORG", "ORGANIZATION"]:
                        organizations.append(entity_text)
                    elif entity_type in ["LOC", "LOCATION", "GPE"]:
                        locations.append(entity_text)
        
        # Ak NER nie je dostupn√Ω, pou≈æijeme alternat√≠vne met√≥dy
        if not has_ner:
            # Extrahuje vlastn√© men√° (PROPN) ako potenci√°lnych ƒæud√≠ alebo organiz√°cie
            # Ignorujeme "Adam" - je to u≈æ√≠vateƒæ, ktor√Ω p√≠≈°e o sebe
            ignored_names = ["adam", "adama", "adamovi", "adamom"]  # R√¥zne sklo≈àovania
            
            for sentence in doc.sentences:
                for word in sentence.words:
                    if word.upos == "PROPN" and len(word.text) > 2:
                        word_lower = word.text.lower()
                        
                        # Ignoruj "Adam" a jeho sklo≈àovania
                        if word_lower in ignored_names:
                            continue
                        
                        # Sk√∫si identifikova≈• podƒæa kontextu
                        # Zn√°me men√° (m√¥≈æe≈° roz≈°√≠ri≈•)
                        known_names = ["vlado", "petr", "laura"]
                        if word_lower in known_names:
                            people.append(word.text)
                        else:
                            # Inak to m√¥≈æe by≈• organiz√°cia alebo meno
                            # Pre jednoduchos≈• prid√°me do people (m√¥≈æe≈° upravi≈•)
                            if word.text[0].isupper():
                                people.append(word.text)
        
        # V≈ædy sk√∫si n√°js≈• technol√≥gie v celom texte
        text_lower = text.lower()
        for keyword in tech_keywords:
            if keyword in text_lower:
                # N√°jde kontext okolo kƒæ√∫ƒçov√©ho slova
                idx = text_lower.find(keyword)
                if idx >= 0:
                    start = max(0, idx - 10)
                    end = min(len(text), idx + len(keyword) + 10)
                    context = text[start:end]
                    # Extrahuje slovo (mo≈æno s veƒæk√Ωm p√≠smenom)
                    words = context.split()
                    for word in words:
                        if keyword in word.lower() and word not in technologies:
                            # Vyƒçisti slovo (odstr√°ni interpunkciu)
                            clean_word = word.strip('.,!?;:()[]{}"\'')
                            if clean_word and len(clean_word) > 2:
                                technologies.append(clean_word)
        
        return {
            "people": list(set(people)),  # Odstr√°ni duplik√°ty
            "organizations": list(set(organizations)),
            "locations": list(set(locations)),
            "technologies": list(set(technologies)),
            "raw": None  # Stanza neposkytuje raw JSON
        }
    except Exception as e:
        logger.warning(f"Chyba pri NER: {e}")
        return None


def extract_concepts(text: str, stanza_nlp) -> Optional[Dict]:
    """Extrahuje kƒæ√∫ƒçov√© pojmy pomocou Stanza (noun phrases a v√Ωznamn√© slov√°)."""
    if not stanza_nlp:
        return None
    
    try:
        doc = stanza_nlp(text)
        concepts = []
        
        # Extrahuje podstatn√© men√° a v√Ωznamn√© fr√°zy
        for sentence in doc.sentences:
            for word in sentence.words:
                # Filtruje podstatn√© men√°, vlastn√© men√° a v√Ωznamn√© slov√°
                if word.upos in ['NOUN', 'PROPN'] and len(word.text) > 3:
                    # Pou≈æije lemma (z√°kladn√Ω tvar) namiesto sklo≈àovan√©ho tvaru
                    lemma = word.lemma if hasattr(word, 'lemma') and word.lemma else word.text
                    concepts.append(lemma.lower())
        
        # Odstr√°ni duplik√°ty a vr√°ti top 20
        unique_concepts = list(set(concepts))[:20]
        
        return {
            "concepts": unique_concepts,
            "raw": None
        }
    except Exception as e:
        logger.warning(f"Chyba pri extrakcii pojmov: {e}")
        return None


def analyze_prompt_with_local_nlp(prompt_data: Dict, stanza_nlp, sentiment_pipeline) -> Dict:
    """
    Analyzuje jeden prompt pomocou lok√°lnych NLP n√°strojov.
    
    Args:
        prompt_data: Dict s prompt_id, text (surov√Ω text)
        stanza_nlp: Stanza pipeline objekt
        sentiment_pipeline: Hugging Face sentiment pipeline
    
    Returns:
        Dict s v√Ωsledkami anal√Ωzy
    """
    # Pou≈æijeme surov√Ω text priamo
    text_to_analyze = prompt_data.get("text", "")
    
    if not text_to_analyze.strip():
        logger.warning(f"‚ö†Ô∏è  Pr√°zdny text pre {prompt_data.get('prompt_id')}")
        text_to_analyze = ""
    
    # Konvertuj date na string ak je datetime objekt
    date_value = prompt_data.get("date")
    if isinstance(date_value, datetime):
        date_str = date_value.strftime("%Y-%m-%d")
    else:
        date_str = str(date_value) if date_value else ""
    
    results = {
        "prompt_id": prompt_data.get("prompt_id"),
        "date": date_str,
        "timestamp": prompt_data.get("timestamp", ""),
        "word_count": prompt_data.get("word_count", 0),
    }
    
    # Extrakcia sentimentu
    logger.info(f"  üìä Analyzujem sentiment...")
    sentiment_result = extract_sentiment(text_to_analyze, sentiment_pipeline)
    if sentiment_result:
        results["sentiment"] = sentiment_result.get("sentiment", "neutral")
        results["sentiment_score"] = sentiment_result.get("sentiment_score", 0.0)
    else:
        results["sentiment"] = None
        results["sentiment_score"] = None
    
    # Extrakcia ent√≠t
    logger.info(f"  üîç Extrahujem ent√≠t...")
    entities_result = extract_entities(text_to_analyze, stanza_nlp)
    if entities_result:
        results["people"] = entities_result.get("people", [])
        results["organizations"] = entities_result.get("organizations", [])
        results["locations"] = entities_result.get("locations", [])
        results["technologies"] = entities_result.get("technologies", [])
    else:
        results["people"] = []
        results["organizations"] = []
        results["locations"] = []
        results["technologies"] = []
    
    # Extrakcia pojmov
    logger.info(f"  üí° Extrahujem pojmy...")
    concepts_result = extract_concepts(text_to_analyze, stanza_nlp)
    if concepts_result:
        results["concepts"] = concepts_result.get("concepts", [])
    else:
        results["concepts"] = []
    
    results["analyzed_at"] = datetime.now().isoformat()
    
    return results


def load_historical_prompts() -> List[Dict]:
    """Naƒç√≠ta v≈°etky historick√© prompty z prompts_split."""
    prompts = []
    
    for day_dir in sorted(PROMPTS_SPLIT_DIR.glob("*")):
        if not day_dir.is_dir():
            continue
        
        for json_file in sorted(day_dir.glob("*.json")):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if not data.get("text"):
                    continue
                
                date_str = data.get("date", day_dir.name)
                try:
                    date = datetime.strptime(date_str, "%Y-%m-%d")
                except:
                    continue
                
                word_count = data.get("word_count", 0)
                
                prompts.append({
                    "prompt_id": f"{day_dir.name}_{json_file.stem}",
                    "date": date,
                    "timestamp": data.get("timestamp", ""),
                    "text": data.get("text", ""),
                    "word_count": word_count,
                })
            except Exception as e:
                logger.warning(f"Chyba pri naƒç√≠tan√≠ {json_file}: {e}")
                continue
    
    logger.info(f"‚úÖ Naƒç√≠tan√Ωch {len(prompts)} historick√Ωch promptov")
    return prompts


def load_current_prompts() -> List[Dict]:
    """Naƒç√≠ta aktu√°lne prompty z prompts_log.jsonl."""
    prompts = []
    
    if not PROMPTS_LOG_PATH.exists():
        return prompts
    
    try:
        with open(PROMPTS_LOG_PATH, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                
                try:
                    data = json.loads(line)
                    
                    if data.get("role") != "user":
                        continue
                    
                    timestamp_str = data.get("timestamp", "")
                    try:
                        if '+' in timestamp_str or timestamp_str.endswith('Z'):
                            date = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                        else:
                            date = datetime.fromisoformat(timestamp_str)
                    except:
                        continue
                    
                    text = data.get("content", "")
                    word_count = len(text.split())
                    
                    prompts.append({
                        "prompt_id": f"current_{line_num}",
                        "date": date,
                        "timestamp": timestamp_str,
                        "text": text,
                        "word_count": word_count,
                    })
                except json.JSONDecodeError as e:
                    logger.warning(f"Chyba pri parsovan√≠ riadku {line_num}: {e}")
                    continue
    except Exception as e:
        logger.error(f"Chyba pri naƒç√≠tan√≠ {PROMPTS_LOG_PATH}: {e}")
    
    logger.info(f"‚úÖ Naƒç√≠tan√Ωch {len(prompts)} aktu√°lnych promptov")
    return prompts


def filter_prompts_by_length(prompts: List[Dict], max_words: int = MAX_WORDS) -> List[Dict]:
    """Filtruje prompty podƒæa dƒ∫≈æky."""
    filtered = [p for p in prompts if p.get("word_count", 0) < max_words]
    logger.info(f"üîç Filtrovan√Ωch {len(filtered)} promptov z {len(prompts)} (limit: {max_words} slov)")
    return filtered


def load_prompts() -> List[Dict]:
    """Naƒç√≠ta v≈°etky prompty z surov√Ωch d√°t (prompts_split + prompts_log)."""
    logger.info("üìñ Naƒç√≠tavam surov√© prompty...")
    historical_prompts = load_historical_prompts()
    current_prompts = load_current_prompts()
    all_prompts = historical_prompts + current_prompts
    
    # Filtruj podƒæa dƒ∫≈æky
    filtered_prompts = filter_prompts_by_length(all_prompts, MAX_WORDS)
    
    return filtered_prompts


def load_existing_analyses() -> set:
    """Naƒç√≠ta u≈æ existuj√∫ce prompt_id z output s√∫boru (pre resume functionality)."""
    existing = set()
    
    if not OUTPUT_FILE.exists():
        return existing
    
    try:
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    data = json.loads(line)
                    prompt_id = data.get("prompt_id")
                    if prompt_id:
                        existing.add(prompt_id)
                except:
                    continue
    except Exception as e:
        logger.warning(f"Chyba pri naƒç√≠tan√≠ existuj√∫cich anal√Ωz: {e}")
    
    logger.info(f"‚úÖ Naƒç√≠tan√Ωch {len(existing)} u≈æ spracovan√Ωch promptov")
    return existing


def save_analysis(analysis_data: Dict, output_path: Path):
    """Ulo≈æ√≠ jednu anal√Ωzu do JSONL s√∫boru."""
    try:
        with open(output_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(analysis_data, ensure_ascii=False) + '\n')
            f.flush()
    except Exception as e:
        logger.error(f"Chyba pri ukladan√≠ anal√Ωzy: {e}")
        raise


def main():
    """Hlavn√° funkcia."""
    logger.info("="*80)
    logger.info("Anal√Ωza promptov pomocou lok√°lnych NLP n√°strojov")
    logger.info("="*80)
    
    # Vytvor output adres√°r ak neexistuje
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    
    # Inicializuj NLP pipeline
    logger.info("üîß Inicializujem NLP n√°stroje...")
    stanza_nlp = init_stanza()
    sentiment_pipeline = init_sentiment_pipeline()
    
    if not stanza_nlp:
        logger.error("‚ùå Stanza pipeline sa nepodarilo inicializova≈•!")
        logger.error("   Sk√∫s: python3 -c 'import stanza; stanza.download(\"sk\")'")
        sys.exit(1)
    
    if not sentiment_pipeline:
        logger.warning("‚ö†Ô∏è  Sentiment pipeline sa nepodarilo inicializova≈•!")
        logger.warning("   Anal√Ωza sentimentu nebude dostupn√°.")
    
    # Naƒç√≠taj prompty
    logger.info("üìñ Naƒç√≠tavam prompty...")
    all_prompts = load_prompts()
    
    if not all_prompts:
        logger.error("‚ùå ≈Ωiadne prompty na spracovanie!")
        return
    
    # Naƒç√≠taj u≈æ spracovan√©
    existing_ids = load_existing_analyses()
    
    # Filtruj u≈æ spracovan√©
    prompts_to_process = [p for p in all_prompts if p.get("prompt_id") not in existing_ids]
    logger.info(f"üìä Zost√°va spracova≈• {len(prompts_to_process)} promptov")
    
    # Test mode - len prv√Ωch N promptov
    if TEST_MODE:
        prompts_to_process = prompts_to_process[:TEST_LIMIT]
        logger.info(f"üß™ TEST MODE: Spracujem len prv√Ωch {TEST_LIMIT} promptov")
    
    if not prompts_to_process:
        logger.info("‚úÖ V≈°etky prompty u≈æ boli spracovan√©!")
        return
    
    # Spracuj prompty
    logger.info("üöÄ Zaƒç√≠nam anal√Ωzu pomocou lok√°lnych NLP n√°strojov...")
    processed = 0
    failed = 0
    start_time = time.time()
    
    for i, prompt in enumerate(prompts_to_process, 1):
        prompt_id = prompt.get("prompt_id", "unknown")
        
        logger.info(f"[{i}/{len(prompts_to_process)}] Spracov√°vam {prompt_id}...")
        
        # Analyzuj prompt
        try:
            analysis = analyze_prompt_with_local_nlp(prompt, stanza_nlp, sentiment_pipeline)
            
            # Ulo≈æ
            save_analysis(analysis, OUTPUT_FILE)
            processed += 1
            
            # Zobraz v√Ωsledky
            sentiment = analysis.get("sentiment", "N/A")
            people_count = len(analysis.get("people", []))
            tech_count = len(analysis.get("technologies", []))
            concepts_count = len(analysis.get("concepts", []))
            
            logger.info(f"‚úÖ Spracovan√©: sentiment={sentiment}, people={people_count}, tech={tech_count}, concepts={concepts_count}")
        except Exception as e:
            failed += 1
            logger.error(f"‚ùå Zlyhalo spracovanie {prompt_id}: {e}")
            import traceback
            logger.debug(traceback.format_exc())
        
        # Progress update ka≈æd√Ωch BATCH_SIZE promptov
        if i % BATCH_SIZE == 0:
            elapsed = time.time() - start_time
            avg_time = elapsed / i
            remaining = (len(prompts_to_process) - i) * avg_time
            logger.info(f"üìä Progress: {i}/{len(prompts_to_process)} ({i/len(prompts_to_process)*100:.1f}%) | "
                       f"ƒåas: {elapsed:.1f}s | Zost√°va: ~{remaining:.1f}s")
    
    # Fin√°lny report
    total_time = time.time() - start_time
    logger.info("="*80)
    logger.info("V√ùSLEDKY")
    logger.info("="*80)
    logger.info(f"‚úÖ √öspe≈°ne spracovan√Ωch: {processed}")
    logger.info(f"‚ùå Zlyhalo: {failed}")
    logger.info(f"‚è±Ô∏è  Celkov√Ω ƒças: {total_time:.1f}s ({total_time/60:.1f} min√∫t)")
    logger.info(f"üìÅ V√Ωstup: {OUTPUT_FILE}")
    logger.info(f"üìã Error log: {ERROR_LOG}")


if __name__ == "__main__":
    main()


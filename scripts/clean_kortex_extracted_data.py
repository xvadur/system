#!/usr/bin/env python3
"""
ÄŒistenie extrahovanÃ½ch dÃ¡t z Kortex backupu.
- OdstrÃ¡nenie duplikÃ¡tov
- OdstrÃ¡nenie prÃ¡zdnych zÃ¡znamov
- Filtrovanie prÃ­liÅ¡ krÃ¡tkych alebo prÃ­liÅ¡ dlhÃ½ch textov
"""

import json
import sys
import hashlib
from pathlib import Path
from typing import Dict, List, Set
from collections import defaultdict

workspace_root = Path(__file__).parent.parent
input_dir = workspace_root / "xvadur" / "data" / "kortex_extracted"
output_dir = workspace_root / "xvadur" / "data" / "kortex_cleaned"

output_dir.mkdir(parents=True, exist_ok=True)

print("ğŸ§¹ ÄŒistenie extrahovanÃ½ch dÃ¡t z Kortex backupu\n")
print(f"ğŸ“ Input: {input_dir}")
print(f"ğŸ“ Output: {output_dir}\n")

# KonfigurÃ¡cia filtrov
MIN_WORDS = 3  # MinimÃ¡lny poÄet slov
MAX_WORDS = 50000  # MaximÃ¡lny poÄet slov (prÃ­liÅ¡ dlhÃ© odpovede)


def hash_text(text: str) -> str:
    """VytvorÃ­ hash z textu pre detekciu duplikÃ¡tov."""
    # Normalizujeme text: lowercase, odstrÃ¡nenie whitespace
    normalized = " ".join(text.lower().split())
    return hashlib.md5(normalized.encode('utf-8')).hexdigest()


def is_valid_text(text: str) -> bool:
    """Skontroluje, Äi je text validnÃ½ (nie prÃ¡zdny, mÃ¡ minimÃ¡lnu dÄºÅ¾ku)."""
    if not text or not text.strip():
        return False
    
    words = text.split()
    word_count = len(words)
    
    if word_count < MIN_WORDS:
        return False
    
    if word_count > MAX_WORDS:
        return False
    
    return True


def clean_user_prompts() -> Dict:
    """VyÄistÃ­ user prompty."""
    print("ğŸ“ ÄŒistÃ­m user prompty...")
    
    input_file = input_dir / "user_prompts.jsonl"
    output_file = output_dir / "user_prompts_cleaned.jsonl"
    
    seen_hashes: Set[str] = set()
    cleaned_count = 0
    duplicates_count = 0
    empty_count = 0
    too_short_count = 0
    total_count = 0
    
    word_counts = []
    
    with open(input_file, 'r', encoding='utf-8') as f_in, \
         open(output_file, 'w', encoding='utf-8') as f_out:
        
        for line in f_in:
            total_count += 1
            data = json.loads(line)
            
            text = data.get("extracted_text", "")
            
            # PreskoÄÃ­me prÃ¡zdne
            if not text:
                empty_count += 1
                continue
            
            # Kontrola validity
            if not is_valid_text(text):
                too_short_count += 1
                continue
            
            # Kontrola duplikÃ¡tov
            text_hash = hash_text(text)
            if text_hash in seen_hashes:
                duplicates_count += 1
                continue
            
            seen_hashes.add(text_hash)
            
            # PridÃ¡me hash do dÃ¡t
            data["text_hash"] = text_hash
            
            # ZapÃ­Å¡eme
            f_out.write(json.dumps(data, ensure_ascii=False) + "\n")
            cleaned_count += 1
            
            word_count = data.get("word_count", len(text.split()))
            word_counts.append(word_count)
    
    avg_words = sum(word_counts) / len(word_counts) if word_counts else 0
    
    print(f"  âœ… Celkom: {total_count}")
    print(f"  âœ… VyÄistenÃ½ch: {cleaned_count}")
    print(f"  âœ… DuplikÃ¡tov: {duplicates_count}")
    print(f"  âœ… PrÃ¡zdnych: {empty_count}")
    print(f"  âœ… PrÃ­liÅ¡ krÃ¡tkych: {too_short_count}")
    print(f"  âœ… PriemernÃ¡ dÄºÅ¾ka: {avg_words:.1f} slov")
    print(f"  âœ… UloÅ¾enÃ©: {output_file.name}\n")
    
    return {
        "total": total_count,
        "cleaned": cleaned_count,
        "duplicates": duplicates_count,
        "empty": empty_count,
        "too_short": too_short_count,
        "avg_words": avg_words,
        "unique_texts": len(seen_hashes),
    }


def clean_ai_responses() -> Dict:
    """VyÄistÃ­ AI odpovede."""
    print("ğŸ¤– ÄŒistÃ­m AI odpovede...")
    
    input_file = input_dir / "ai_responses.jsonl"
    output_file = output_dir / "ai_responses_cleaned.jsonl"
    
    seen_hashes: Set[str] = set()
    cleaned_count = 0
    duplicates_count = 0
    empty_count = 0
    too_short_count = 0
    total_count = 0
    
    word_counts = []
    
    with open(input_file, 'r', encoding='utf-8') as f_in, \
         open(output_file, 'w', encoding='utf-8') as f_out:
        
        for line in f_in:
            total_count += 1
            data = json.loads(line)
            
            text = data.get("extracted_text", "")
            
            # PreskoÄÃ­me prÃ¡zdne
            if not text:
                empty_count += 1
                continue
            
            # Kontrola validity
            if not is_valid_text(text):
                too_short_count += 1
                continue
            
            # Kontrola duplikÃ¡tov
            text_hash = hash_text(text)
            if text_hash in seen_hashes:
                duplicates_count += 1
                continue
            
            seen_hashes.add(text_hash)
            
            # PridÃ¡me hash do dÃ¡t
            data["text_hash"] = text_hash
            
            # ZapÃ­Å¡eme
            f_out.write(json.dumps(data, ensure_ascii=False) + "\n")
            cleaned_count += 1
            
            word_count = data.get("word_count", len(text.split()))
            word_counts.append(word_count)
    
    avg_words = sum(word_counts) / len(word_counts) if word_counts else 0
    
    print(f"  âœ… Celkom: {total_count}")
    print(f"  âœ… VyÄistenÃ½ch: {cleaned_count}")
    print(f"  âœ… DuplikÃ¡tov: {duplicates_count}")
    print(f"  âœ… PrÃ¡zdnych: {empty_count}")
    print(f"  âœ… PrÃ­liÅ¡ krÃ¡tkych: {too_short_count}")
    print(f"  âœ… PriemernÃ¡ dÄºÅ¾ka: {avg_words:.1f} slov")
    print(f"  âœ… UloÅ¾enÃ©: {output_file.name}\n")
    
    return {
        "total": total_count,
        "cleaned": cleaned_count,
        "duplicates": duplicates_count,
        "empty": empty_count,
        "too_short": too_short_count,
        "avg_words": avg_words,
        "unique_texts": len(seen_hashes),
    }


def clean_conversation_pairs() -> Dict:
    """VyÄistÃ­ konverzaÄnÃ© pÃ¡ry."""
    print("ğŸ”— ÄŒistÃ­m konverzaÄnÃ© pÃ¡ry...")
    
    input_file = input_dir / "conversation_pairs.jsonl"
    output_file = output_dir / "conversation_pairs_cleaned.jsonl"
    
    # NaÄÃ­tame hash mapu vyÄistenÃ½ch user promptov a AI odpovedÃ­
    print("  ğŸ“– NaÄÃ­tavam hash mapy z vyÄistenÃ½ch dÃ¡t...")
    
    user_hashes: Set[str] = set()
    ai_hashes: Set[str] = set()
    
    # User prompty
    cleaned_user_file = output_dir / "user_prompts_cleaned.jsonl"
    if cleaned_user_file.exists():
        with open(cleaned_user_file, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                text_hash = data.get("text_hash")
                if text_hash:
                    user_hashes.add(text_hash)
    
    # AI odpovede
    cleaned_ai_file = output_dir / "ai_responses_cleaned.jsonl"
    if cleaned_ai_file.exists():
        with open(cleaned_ai_file, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                text_hash = data.get("text_hash")
                if text_hash:
                    ai_hashes.add(text_hash)
    
    print(f"  âœ… NaÄÃ­tanÃ½ch {len(user_hashes)} user prompt hashov")
    print(f"  âœ… NaÄÃ­tanÃ½ch {len(ai_hashes)} AI odpoveÄ hashov")
    
    # Teraz ÄistÃ­me pÃ¡ry
    seen_pair_hashes: Set[str] = set()
    cleaned_count = 0
    duplicates_count = 0
    invalid_count = 0
    missing_user_count = 0
    missing_ai_count = 0
    total_count = 0
    
    with open(input_file, 'r', encoding='utf-8') as f_in, \
         open(output_file, 'w', encoding='utf-8') as f_out:
        
        for line in f_in:
            total_count += 1
            data = json.loads(line)
            
            user_prompt = data.get("user_prompt", {})
            ai_response = data.get("ai_response", {})
            
            user_text = user_prompt.get("extracted_text", "")
            ai_text = ai_response.get("extracted_text", "")
            
            # PreskoÄÃ­me prÃ¡zdne pÃ¡ry
            if not user_text or not ai_text:
                invalid_count += 1
                continue
            
            # Kontrola, Äi oba texty sÃº v cleaned dÃ¡tach
            user_hash = hash_text(user_text)
            ai_hash = hash_text(ai_text)
            
            if user_hash not in user_hashes:
                missing_user_count += 1
                continue
            
            if ai_hash not in ai_hashes:
                missing_ai_count += 1
                continue
            
            # Kontrola duplikÃ¡tov pÃ¡rov
            pair_hash = hashlib.md5((user_hash + ai_hash).encode()).hexdigest()
            if pair_hash in seen_pair_hashes:
                duplicates_count += 1
                continue
            
            seen_pair_hashes.add(pair_hash)
            
            # PridÃ¡me hash do dÃ¡t
            data["pair_hash"] = pair_hash
            data["user_prompt"]["text_hash"] = user_hash
            data["ai_response"]["text_hash"] = ai_hash
            
            # ZapÃ­Å¡eme
            f_out.write(json.dumps(data, ensure_ascii=False) + "\n")
            cleaned_count += 1
    
    print(f"  âœ… Celkom: {total_count}")
    print(f"  âœ… VyÄistenÃ½ch: {cleaned_count}")
    print(f"  âœ… DuplikÃ¡tov: {duplicates_count}")
    print(f"  âœ… NevalidnÃ½ch: {invalid_count}")
    print(f"  âœ… ChÃ½bajÃºci user prompt: {missing_user_count}")
    print(f"  âœ… ChÃ½bajÃºca AI odpoveÄ: {missing_ai_count}")
    print(f"  âœ… UloÅ¾enÃ©: {output_file.name}\n")
    
    return {
        "total": total_count,
        "cleaned": cleaned_count,
        "duplicates": duplicates_count,
        "invalid": invalid_count,
        "missing_user": missing_user_count,
        "missing_ai": missing_ai_count,
    }


def main():
    """HlavnÃ¡ funkcia."""
    
    # ÄŒistÃ­me v poradÃ­: user prompty, AI odpovede, pÃ¡ry
    user_stats = clean_user_prompts()
    ai_stats = clean_ai_responses()
    pairs_stats = clean_conversation_pairs()
    
    # SÃºhrn
    print("=" * 60)
    print("ğŸ“Š SÃšHRN ÄŒISTENIA")
    print("=" * 60)
    
    print(f"\nğŸ“ User prompty:")
    print(f"  {user_stats['cleaned']} / {user_stats['total']} vyÄistenÃ½ch")
    print(f"  {user_stats['duplicates']} duplikÃ¡tov odstrÃ¡nenÃ½ch")
    print(f"  {user_stats['unique_texts']} unikÃ¡tnych textov")
    
    print(f"\nğŸ¤– AI odpovede:")
    print(f"  {ai_stats['cleaned']} / {ai_stats['total']} vyÄistenÃ½ch")
    print(f"  {ai_stats['duplicates']} duplikÃ¡tov odstrÃ¡nenÃ½ch")
    print(f"  {ai_stats['unique_texts']} unikÃ¡tnych textov")
    
    print(f"\nğŸ”— KonverzaÄnÃ© pÃ¡ry:")
    print(f"  {pairs_stats['cleaned']} / {pairs_stats['total']} vyÄistenÃ½ch")
    print(f"  {pairs_stats['duplicates']} duplikÃ¡tov odstrÃ¡nenÃ½ch")
    
    # UloÅ¾Ã­me Å¡tatistiky
    stats_file = output_dir / "cleaning_stats.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump({
            "user_prompts": user_stats,
            "ai_responses": ai_stats,
            "conversation_pairs": pairs_stats,
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ Å tatistiky uloÅ¾enÃ©: {stats_file}")
    print(f"\nğŸ‰ ÄŒistenie dokonÄenÃ©!")
    print(f"ğŸ“ VÃ½sledky: {output_dir}")


if __name__ == "__main__":
    main()

